-20. alignment-cat A.fasta B.fasta C.fasta > D.fasta
     The sequence should have the same names, but could be
      different alphabets.  (Hopefully we will not handle alphabets)

-19. Read Schmidler papers + others from postdoc e-mail.

-18. Read Seo paper + others

-17. trees-consensus: report number of branches.

-16. Compare estimates w/ MrBayes
     a) compare mixing, topologies, branch lengths, speed, etc.
     b) globin / EF-Tu 48b / P1-123 / P1-1234

-15. Check letter classes analysis - does it affect substitution-index.C?
     If not, then checking the first iteration should be enough.

-14. Consider calculating PP of all the trees in alignment-consensus...
     a) This is really slow, so don't keep on RE-calculating everything.
        Just mask in new partitions.

     b) We should be able to use tree structure to narrow search for 
        sub-partitions, since any partitions is ordered with respect
        to a full partition.

-13. Make a program to compare two alignment distributions.
     a) Flag homology pairs that have abs(log(p1/p2)) > thresh-hold. 
     b) Output the pairs of residues that act up.
       b.1) Have a method to convert the pairs to a color scheme.
       b.2) Invent a color scheme that is something like 'grey' (1) +
            red (0). 
     c) Allow the user to specify either a sequence of alignment
        samples or an alignment w/ '?' characters, and a level of
        certainty at which to consider each pair in that alignment.
 
-12. Allow alignment-gild to annotate an alignment-consensus

-11. Don't show SRQ or sum plots for partitions w/ less than <N=10> regenerations,
     a) But, perhaps the real issue is generating a confidence band for SRQ plots
     b) Develop an idea of how to do a statistical test based on SRQ plots...
        [ Find the worst sub-section - the flattest line that is longest ]
        [ This will give us the worst mixing rate that we can reject ]

-10. Make trees-bootstrap.C print trees instead of collections of full partitions.

-9. sub-class DPengine w/ transition matrix implemented as
     A.  i) Matrix  ii) const Matrix& iii) computed Matrix
     B.  But how to compute GQ... ?

-8.  tree-dist.C: store topology as list of locations in partition list?
   a) Would this save memory?  What if only sub-partitions were constant?
   b) Would this save CPU time?  Examine each partition once instead of n times.      [But this wouldn't help for sub-partitions, would it?]

-7. Sanitize use of MAP_tree in tree-dist-compare when the count is 1.   
    Report a median when there are a number of trees w/ the same count?

-6. Benchmarking of mixing... 
  a) try with alignment fixed
  b) look at log autocorrelation and effective sample sizes for distances
     between 2 nodes + sum all branches + sum all paths
  c) compare mixing with few taxa, many taxa...
  d) handle alignment predicates in the SRQ plot.
     can we output residues which are mixing badly?

-5. Speeding up GQ_exit( )
  a) can we avoid copying G into G2 - can we construct it in place?
  b) if we put the check into #ifndef NDEBUG, does this gain us anything?
  c) algorithm currently does more or less S*S*(S+NS) multiply-and-add's.
  d) it also takes time to compute the silent state topology.  Can we cache this?

-4. Actually do the MCMCMC T thingy - how much slowdown does this give?

-3. Think more carefully about using walk_tree_path
    when changing the topology...

-2. Test various ways of improving mixing
  a) more branch length proposals
  b) branch length proposals in more passes? (how does this affect timing?)
  c) propose scaling the entire tree
  d) propose branch lengths on the log scale...
  e) increase the number of SPR moves
  f) look for papers - by largett's student - on tree mixing
    i) swofford, largett, huelsenbeck - ?

-1. Find a way to allow substitution models to propose a lot of different
    changes each time we try to propose new parameters.
    a) ask the model for a number of fiddles to do, and then do
       for(int i=0;i<n_fiddles;i++) {
	 model2 = smodel
         rho = model2->fiddle(i)
         accept/reject(model,model2,rho)
       }

-i.  We need a way of separating priors and proposals from models...
     Hmmmm.

------
0. Plot #topologies in confidence interval (0,p) vs p. (e.g. sort topologies)
    Then plot this versus time...

    Also compare this between runs...

    Figure out burnin for bacteria data sets...

1. Reduce memory usage:
 b) use FastLSA algorithm to change memory usage to O(N+M)*k instead of O(N*M)

2. Try with STLport, mudflap, valgrind, etc.
  a) STLport could actually give us a speedup!

2.5. Make a better framework for reporting acceptance rates.
  a) pass an object to each move to record stats.
  b) each move records stats under a name that it passes.

3. Indel model - fix so that prior probabilities exist!
    a) for now, we condition on leaf sequence lengths.

4. Start making some "nice" pictures to put up on the website.
    a) include pictures to give to Dr. Lake's lab.
    b) include pictures to give to Dr. Sabatti.

5. MCMCMC
   ??) Find a way to correctly run heated chains, avoiding:
       0) ?
       i) improper pairwise alignment distribution
      ii) really expensive DP calculations...
    a) Make the temperature affect only the likelihood...
    b) test speedups...
    c) begin to implement the forking code / temperature identifier.
    d) begin to implement the separate random number generators.

6. Make a more general substitution::Model class, so that we can e.g.
    have trees with 2 sets of branch lengths...

8. Add partitions from M[0.5] to M[l] if they are revealed by deleting subtrees.
   a) also mark "badly-rooted" subtrees?  (edges are hidden because of badly-rooted-ness?)

10: If we are only changing bin frequencies in the smodel, we shouldn't have
    to recalculate all the conditional likelihoods.  How to implement this?
    Some kind of fiddle_safe() routine?

11. Improve mixing for multi_freq model by sampling between columns.
    a) well, firstly, why does convergence take SOO long?
    b) if we could easily just change bin probabilities, then that would
       make it easier to attack this problem by increasing the number
       of MH steps we do.

12. Make a hash function for sub-partitions that is 
     (hash(left mask) * hash(right mask)), where the hash() function
     is a hash function for bitmasks and involves addition, not
     multiplication. 

13. Add partitions to M[0.5] that imply l=0.5 sub-partitions.


------
A. sub-partitions: 

*B. How to make priors specified on command line?

C. Debugging framework 
   a) platforms
     1) intel c++
     2) cygwin
     3) MS Visual C++
  b) stlport
  c) compare
     1) MrBayes
     2) bambe
     3) BEAST
  d) mudflap -> alter GNUmakefile so that this is easy

*D. Make alignment-draw be able to output EPS

E. how does P(data|model) depend on number of frequency bins?

F. Allow alignment to contain only 2 sequences?

G. Find a better distance measure for alignment-median?

*H. implement: new framework for fixing things
	a) disable=alignment  -> fixed=A fixed=alignment
	b) disable=alignment_branch -> fixed=A- fixed=leaves-alignment
	c) disable=tree             -> fixed=tau fixed=topology
	d) disable=frequencies      -> fixed=pi  fixed=frequencies

------------------------------



I. fix: gamma+frequency+INV

II. model: Make INV categories depend on letters, somehow...

III. sample lengths/topologies MORE often when alignment sampling also?

IV. A new ggobi is available!  Try multidimensional scaling...

V. Start proposing INV::p on the log odds scale. It is OK to change
the prior, because we never want to use it  differently.


------------------------------------------------------------------------------
[FIXED] s-parameters aren't accepted/sampled often enough.

[FIXED] implement: 'f' parameter in ReversibleMarkov models

[FIXED] check: memory usage -> EF-Tu/12d ~ 120 Mb RAM ?
        (memory usage decreased)

[DONE]  make a separate DEBUG_CACHING

[DONE]  make change_parameters() debugging code call a function which outputs
        substitution parameters.

[DONE]  allow comments in the alignment constraint file, and make
        the alignment index program spit out the actual letters for each
        column as comments. 

[DONE]  Make tree-dist-compare consider all partitions w/ PP > 0.5

[DONE]  Stop using PHYLIP.  Dealing with truncated names is too painful!

[DONE]  alignment-gild: use list<alignment> instead of vector<>...

[DONE]  alignment-draw: implement bg-colors=type, bg-whiteness = uncertainty

[DONE]  Use boost::program_options and remove arguments.{H,C}

[DONE]  Estimate best alignment by finding the alignment with the greatest
   sum of PP(i ~ j) for all aligned pairs (i,j);

[DONE]  Fix "showonly" - think of better name. (urgh - picked show-only)

[DONE]  Fix setting of 'fixed' flags in model.H/C, to allow 'unfixing' parameters
        and also modifying fixed/unfixed for sub-models.

[DONE]  Fix smodel parameters aren't being sampled.

[DONE]  Make A3-stripped.fasta work with alphabet=Codons

[DONE]  Implement YangM2

[DONE]  Stop requiring --random-tree-ok


[DONE]  Implement sorting of subA columns to provide stable names when columns are re-ordered.
[DONE]  Remove invalidation in 3way sampling, 5way sampling, and tri-sampling.

[DONE]  tree-dist-compare: 
	a) by default, only show 1 topology per file
	b) allow consensus to take a comma-separated list of values

[DONE] add '--first' and '--last' arguments to alignment-find

[DONE] Update tools to use boost::program_options.

[DONE] Fix tools/analyze-distances: make an interface to get likelihoods by column...

[DONE] Record when each constraint is first satisfied, and when all are satisfied
[DONE] Make align-constraint= work SPR_and_A

[DONE] Get intel compiler

[DONE] alignment-draw: put all colormaps on the same scale by making colormaps
       nested and colorschemes not nested.

[DONE] alignment-draw: allow names like --color-scheme=AA+contrast+whiten --scale LOD

[DONE] Allow name[arg] formath in smodel description -> Empirical[wag] + gamma[4]

[DONE] Make alignment-draw simply use 'plain' if AU-file not specified.

[DONE] Better multi_freq model.  
    * frequency will be the frequency of the sub-model.
    * sum_l a(m,l) = 1
    * distribution(m) = sum_; a(m,l)*f(l)
    * f(m,l) = a(m,l) * f(l) / distribution(m)
    * two moves:
      - fiddle frequencies (as normal)
      - fiddle each row (one letter, all models)
    * prior -> each row is dirichlet

[DONE] 	Integrating joint-A-T and the 'hack' to set indel prior parameters.

        
[DONE] Report constraints satisfied by initial alignment/tree.

[DONE] Don't report constraints satisfied if there aren't any.

[DONE] Change DualModel to MixtureModel[n]
        a) make a way of specifying the priors

[DONE] Fix gamma model so that we can put MultiModels underneath it w/o crashing!

[DONE] Convert probabilities to efloat_t in substitution, model priors, etc.

[DONE] Make standardized checks for indexing problems in sample-node / sample-two-nodes.

[DONE] Rename YangCodonModel to YangM0

[DONE] Using regular FP math for DP, with a scaling factor per cell.
       * move created of std::vector< > temporary variables out of loops.
       * switched to using std::valarray< > instead of std::vector< > 
         for state_{array,matrix}
       * inlined di,dj,dk,dl,dc in 3way.H from 3way.C
       * stopped copying a whole bunch of state_{array,matrix}'s around
       * move initialization of s12_sub(,) to forward_cell( ).

[DONE] DPmatrix speedup - phase 2
       * add a boundary, and shift everything by (+1,+1)
       i then we can remove check to optionally break out of the inner loop
       * side effect is that we initialize as we go, even states whose predecessor
         is out of the square.
       * initialize boundaries in forward_square, with special case for (1,1)=S(0,0)
       * special case sample_path() for DPmatrixConstrained( ) because we don't initialize
         illegeal state / location pairs.
      ii REMOVE THE FULL INITIALIZATION in DPmatrix::DPmatrix( )!

       Also:
       * shift s1_sub, s2_sub, s12_sub to matrix coordinates by adding padding to the beginning.

[DONE] Fix sub-alignment indices
  a) make A?::construct reconstruct subA indices
     - indices are uniquely defined - reconstructing from scratch
       should not change them.
     - indices are dependant only on leaf-taxa alignment & tree
  b) reconstruct subA indices when tree changes 
     - sample-tri.C for sample-topology-SPR.C

[DONE] Don't store more that 1 DPmatrix at a time - free memory afterwards unless debugging.

[DONE] Add asymmetric MH proposal capability.

[DONE] Convert all densities to use log_double_t for *_pdf().

[DONE] Implement asymmetric proposals for sample_*_multi( );

  b) implement checking when using this routine

[DONE] Implement choose_MH( ).

[DONE] Write alignment-consensus.

[DONE] Start fiddling the 'f' parameter, but making it fixed by default.

[DONE] Allow specifying a non-HKY model underneath the Yang M0 model.

[DONE] (a) Estimate branch lengths for multifurcating trees
       (b) Pass the c50 tree w/ lengths to alignment-reorder

[DONE] Given a c50 tree, keep partitions that imply sub-partitions.
       If two partitions imply a sub-partitions, only keep the one
         with the smallest min(count(group1),count(group2))

[DONE] tree_sample cleanup part I.
       * Store leaf-sized bitmasks per branch.
       * Store leaf names once per tree_sample, not per-tree.
       * Reduces 29Mb -> 6Mb (factor of 5 decrease)

[DONE] Call get_Ml_sub_partitions_and_counts only ONCE.
       * pull out combinations of badly rooted branches.
       * pull out (A,B):-X as (-A-X) and (-B-X)
       * cache computed PP for all sub-partitions,
       * find M[l] branches at each level l in the cache

[DONE] Plot the number of supported branches for EVERY level.  
       Then we could branches vs level, which is less sensitive
        than looking at presence or absences at a SINGLE level.

[DONE] Do a better job at finding the minimal number of c50 branches
        that imply a partition.

[DONE] Make a 'Mixing' directory in addition to 'Results' and 'Work'.

[DONE] tree-to-srq: take partitions as input, not just trees.

[DONE] Make a ??? letter for Codon alphabets.

[DONE] Remove '--with-stop', and add new alpabet names "Amino Acids + stop"
       and "Codons + stop"  

[DONE] Print blocksize, seed, and pseudo count on the same line in [ ].

[DONE] Separate tree-dist-compare into two programs: 
        a) trees-consensus estimates MAP and M[l] topologies for a
           single tree sample 
        b) trees-bootstrap bootstraps support for partitions, possibly
           from multiple tree samples.
          i) speed up the trees-bootstrap by using the same samples
             for each predicate, so that we need only generate the
             random numbers once.

[DONE] Print N or X instead of *

[DONE] Stop computing leaf partition sets.
       Only compute+cache partition masks when we need them.

[DONE] Cleanup "unused parameter" and other warnings.

[DONE] trees-bootstrap:
       
     a) make the number of samples in the bootstrap a parameter
     b) use pseudocounts if specified.

[DONE] Make tree a virtual public base class for SequenceTree and RootedTree?

[DONE] Implement letter classes.
       a) Allow reading Codon alphabet w/ individual letter wildcards.
       b) Handle letter classes in parsimony analysis.
       c) Allow letter classes in other places.

[DONE] Put Data/ under version control.

[DONE] alignment-cut

*IDEAS*

 - make a SplitsTree-like graph out of a tree sample,
   where the edges mean bi-partitions, and don't use distance
   information but topology information instead. 
   (contact girl from INCOB?)

 - for HIV data, we want samples from DIFFERENT PATIENTS, not
   from the same patient, because recombination is limitted to
   within-patient samples.

   if we can avoid co-infection, then we are good!

   (a) check Shankarrapa samples from the HIV database...
   (b) ? ask Marc about asking people if we can use their data
       or asking questions about their data.

 - find some papers for the journal club?

 - introduce alphabet models - such as codon model frequencies depending
   on [A],[T],[G],and [C].  We allow them to deviate, but put evidence on
   them NOT deviating.

 - consider doing variable amino-acid frequencies in a CODON model!

 - consider modelling functional divergence - Karin's branch problem?

 - for alignment - how do we balance "extra high rate of change on this branch"
   (e.g. different aa on each side ) with "unalign the two sides of this
   branch"?

 - if we can take a <X>% confidence region in tree space, then any strict
   consensus measure of that region would be an level-<X> consensus tree.

 - we could search for the <X>% of the samples that would give the most
   resolved tree.

 - consider a version of pair-HMM with 2x as many states: we would have a 
   "slow" and "fast" version of M,G1,G2 in which indels have different
rates.

 -  more complex model: have 'slow' and 'fast' be properties of the
   individual letters in the state.  (gamma-distributed indel rates.)
   + this makes a "spatial" model, if it is done right.

 -  Homoplasy and variation in column frequencies.  
   a) have a 'two-letter' model for HIV sequence DNA.

 - "Autocorrelation" as a function of distance

   o graph distances as a function of the posterior LOD score of the events.

   o graph probability of the transition from 1->0 (or 1->0->1) vs time AND PP.

 - use proxy for Likelihood to propose new states.
   o look at the number of SPR changes required to reach "nearby" trees,
     and to move between "islands".=======
 -  graph distances as a function of the posterior LOD score of the events.

 -  IDEA #1: simulate P(Y|T,tau) as \prod P(Y[i][j]|T,tau)^n[i][j]
            where n[i][j] = 1/2**(D(i,j)-1) 
            wjere D(i,j) is the number of edges between nodes i and j.

 -  IDEA #2: Determine points C[i] as centers for n bins of a distribution.
            + bin boundaries will be at sqrt(C[i]*C{i+1]).
            + define f(x) = C[i] if x is in the i-th bin.
            + then we 
              - constrain E X = 1 under f(x).
              - minimize E |log(X) - log(f(X))|

	    + this will give us bins optimally spaced on the log scale.

 - If we look at posterior rate distributions, then we will want to know
   how the rates are distributed spatially.  (This might explain why the
   shape of the rate distribution varies between proteins.)

   How much of the rate distribution can be explained by e.g. exposure
   to solvent?

   If we add alpha/beta/loop notation to each of the internal node residues,
   then this might help us get more realistic rates, as well as indel
   hot and cold spots.

 - How do indel hot and cold spots map to interior/exterior of 3D structure?

 - Can we label each residue with the probability that it will accept deletion?
   Can we label proposed indels with a relative probability of occurrence?
   (Invariant residues/blocks are easier, because they are never inserted.)
   Can we handle a model with both unequal proposals and acceptance rates?